{"pages":[{"title":"","text":"","link":"/blog/Gallary/index.html"},{"title":"Projects","text":"In Progress Demo for Broker’s websitewww.mai1015.com/projects/realhome About me Page Write in Vanilla Html + Css + Javascriptmxlo.top","link":"/blog/projects/index.html"}],"posts":[{"title":"A structed light based 3d reconstruction using combined circular phase shifting patterns","text":"Presented by Dr. Yujia ZhangExamples: laser scanner kinect -depth from focus ,depth from stereo Concept :active visionPassive triangulation:Stereo Vision Active Triangulation:Structured Light 结构光 one of cameras replaced by light emitter correspondence solved by searching in camera image no geometric constraints Structured light any spatio-temporal pattern of light projected on a surface Classification of structured light patterns time multiplexing high resolution robust only reconstruct static object need large num of patterns Binary Codes for time multiplexing: $2^n$ strips N-ary Codes: decrease num of pattern but increased the basis of code and intensity(grey levels/colors) spatial multiplexing using spatial infos but can produce errors in decoding process maximum res cannot be reached Non-formal codification:periodic pattern De brujin sequences: order m over an alphabet of n symbols in a circular string of length $n^m$ that contains every substring of length m exactly once M-arrays: like de brujin but using sequence matrix can use grey/color level Direct Codification: Reduced pattern ,high res,but noisy and non-linearities low accuracy can use grey/color level High accu ,High res -&gt; phase shift+gray code High accu , Structured light 3d scanningStereo CalibrationPattern Encoding and Decoding single-axis encoding double-axis encoding Coding Methods Binary code $N=log_2(\\frac{w}{h})$ GrayCode $B_iB_{i-1} \\oplus G_i​$ Maximum min-Sw gray code: Phase shifting Methods Micro phase shifting: Decode $R_{micro} = M_{micro} \\cdot U_{micro}$ Triangulation: ray-ray intersection method to get depth 2D circular phase shifting intensity calibration of projector to get gamma response work ray-cone triangulation- using normal alg will get wrong result 44 patterns only for static object for reflection object it can do better job Hybrid Coding Methods gray+phase shifting Min-SW + Micro phase Shifting Maximum Min-SW Gray Code and Circular phase shifting Real Time trackingArUco tracker co-registration system IMU-assisted ArUCo marker tracking","link":"/blog/2019/01/17/A%20structed%20light%20based%203d%20reconstruction%20using%20combined%20circular%20phase%20shifting%20patterns/"},{"title":"A simple example with Socket","text":"A simple example with SocketConcepts: socket: thread I/O stream Output stream -&gt; Sending buffer queue (SendQ)-&gt; Network Network-&gt; Receiving buffer queue (RecvQ)-&gt;Input Stream Example:Write a net program, include a server and client, client will send a string to the server, and server should print the string to the terminal, and return the length of string to the client, and finally the client should print the length sent from server. You should do it in TCP and UDP way. Code:TCP Client12345678910111213141516171819import java.io.*;import java.net.Socket;public class TCPClient { public static void main(String args[]) throws Exception{ Socket socket = new Socket(\"127.0.0.1\",65000); OutputStream os = socket.getOutputStream(); InputStream is = socket.getInputStream(); os.write(new String(\"xxx\").getBytes()); int ch = 0; byte [] buff = new byte[1024]; ch = is.read(buff); String content = new String(buff,0,ch); System.out.println(content); is.close(); os.close(); socket.close(); }} TCP Server12345678910111213141516171819import java.net.ServerSocket;import java.net.Socket;import java.util.SortedSet;public class TCPServer { public static void main(String args[]) throws Exception{ //create and bind port ServerSocket ss = new ServerSocket(65000); //dead loop awaiting client sending request while(true){ Socket socket = ss.accept(); //only start after accept the require from client new LengthCalculator(socket).start(); } }} Length Calculator12345678910111213141516171819202122232425262728293031323334353637383940414243import java.io.*;import java.net.Socket;public class LengthCalculator extends Thread{ private Socket socket; public LengthCalculator(Socket socket){ this.socket = socket; } public void run(){ try{ //output stream OutputStream os = socket.getOutputStream(); //input stream InputStream is = socket.getInputStream(); //ch for reading string length int ch=0; //buffer for input as isread only accpet byte so we have to made buff byte byte[] buff = new byte[1024]; //isread will return the buff's length ch = is.read(buff); String content = new String(buff,0,ch); System.out.println(content); //also os stream only accept array of bytes os.write(String.valueOf(content.length()).getBytes()); //close I/O stream is.close(); os.close(); socket.close(); }catch(Exception e){ System.out.println(e.toString()); } }}","link":"/blog/2019/06/24/A%20simple%20example%20with%20Socket/"},{"title":"Huffman Coding","text":"Method:The Huffman coding which implies the greedy algorithm has its theory: If you have a set of characters , and called it $C$ , has several characters which may or may not overlap each other. Then lets say $C$ contains these characters: f,e,d,c,b,a each character has different frequency which we will called it $F$ . Now we have a sequence like: f(5) ,e(9), d(16), c(12),b(13),a(45) Now if we want to code these characters in order to get a better compression of the space we stored it , we have two choices we just simply code it into same length code we use the greedy method, which means we do our best to put the higher frequency character into short codes and lower frequency characters into the longer codes As these kind of coding can be describe as a tree , and denote as follows: $B(T)=\\sum_{c\\in C }{f(c)d_t(c)}$ $B(T)$: the final coding length $f(c)$: the frequency of the character $d_T(c)$: the depth of character in the tree, which also means the code length For case 1 and 2, we will show in graph a and b apparently case 1 has larger $B(T)$ than case 2. Prove of greedy is the optimized solution for huffman","link":"/blog/2019/07/10/Greedy-Huffman%20Coding/"},{"title":"React-Redux 学习笔记","text":"First of AllRedux并不是全局state管理唯一的方案，基于开发不同的工程，我们可以选择三种方案来解决props传递的问题。 Hook React Context API and Redux store​ store 是本地state的集合，操作的state的都在里面，本地state指的是组件及其子组件的state. reducer​ reducer干的事很简单， 拿到一个state 和一个action返回 newstate ​ 我们可以总结为一个arrow funciton: 1(state,action) =&gt; newState reducer 需要一个初始状态，并且我们需要确保reducer在任何状态下都有state,缺失state的reducer是非常差劲的写法，初始状态示意如下 123const initialState = { something:0,} Dispatch Actions现在我们有state了，需要一个function 来改变action, 这个funciton就叫dispatch, 而action则需要一个object来装。一般而言，只要有叫做type 的property就能叫做action。 123{ type:\"SOMETHING\",} Action描述的是你希望改变的state 而执行action的动作叫做dispatch Dispatch 的运行机制dispatch可以在store对象上运行，比如 1store.dispatch({type:\"Do Something\"}) 但我们需要注意到，实际上每一次store run dispatch 都是在调用 re’ducer 那么话题就回到reducer上了，我们需要在reducer里handle被dispatch的action 我们可以采用比较ugly的风格，比如 12345if(action === \"do things 1\"){ //some codes here}else if(action === \"do things 2\"){ //some codes here} 也可以采用比较好看一点的switch case来完成action的处理。 我们可以注意到，reducer承担的职责更多了，同时也会导致reducer函数的复杂化。那么就引出了话题，我们应该如何写reducer函数。 Reducer的注意事项根据规则的推荐，reducer首先需要是一个无副作用的纯函数。 这就意味着我们不能 随便的碰state这个重要的参数 错误的示范如下 12state.something = 0;state.something++; 那么如何正确的更新state? 我们需要跟随如下规则 State是只读的，只有action能更改他 更新的唯一方式是dispatch(action)-&gt;reducer-&gt;newState Reducer函数必须是无副作用的纯函数，即不能修改他的参数 连接组件与Store现在我们需要把组件和store连接起来，使这个下所有的子组件都可以访问store.做法如下，我们需要引入一个组件叫做Provider 123456789import { Provider } from 'react-redux';//some codes hereconst ParentComponent = () =&gt; ( &lt;Provider store={store}&gt; &lt;ChildComponent/&gt; &lt;/Provider&gt;); 这样，childcomponent 以及 childcomponent的子component都可以正常访问store了 Provider是使用React-Context实现的，比起context他看起来更加友好和易于理解。 在子组件中连接Redux我们首先需要在子组件中引入connect组件。 12345678910import { connect } from 'react-redux';//some other codesfunction mapStateToProps(state){ return{ something:state.something };}//and alsoexport default connect(mapStateToProps)(Counter) 此外需要注意的是，connect是高阶函数或者叫高阶组件 ，他的作用是拿到一个传入参数是函数的时候，返回一个函数，在这个情境下，则是传入一个组件，返回一个新的组件。 至于 mapStateToProps只是一个便于理解的名称，它可以叫任何东西，以及，他只要接受state对象然后返回props对象 我们其实知道react在组件中传递到的都是props 而mapsStateToProps这个function实际的作用是 将需要传递的states map 成 props的形式进行传递 筛选需要传递的states 向子组件中传递Dispatch Actions现在我们的子组件已经能读取到他需要的父组件的state了，我们还需要传递dispatch actions让他能够使用，在子组件里，我们只要添加 1this.props.dispatch({type:&quot;DoSomething&quot;}); 就可以正常使用dispatch actions了 另外，我们完全可以将actions放进名为action.js的文件里 并将我们的actions进行输出 1export const DoSomething = &quot;DoSomething&quot; 更进一步，当我们面对整个项目越来越复杂的actions的时候，action.js,或者叫action生成器将帮助我们走出困境。 正确的使用ACTION生成器我们可以如此正确的使用action.js，除了上文提到的赋值const,我们还可以将action.,js 12export const DoSomething = &quot;DoSomething&quot;export const dosomething = () = &gt;({type:DoSomething}) 使用Redux 请求数据1 Dispatch Action 获取数据","link":"/blog/2019/08/18/React-Redux%20%E5%AD%A6%E4%B9%A0/"},{"title":"React 组件传参","text":"假设我们有一个组件 123456789class Square extends React.Component { render() { return ( &lt;button className=\"square\"&gt; {this.props.num} &lt;/button&gt; ); }} 然后在另一个组件里调用 123456789101112131415161718192021222324252627282930class Board extends React.Component { renderSquare(i) { return &lt;Square num={i}/&gt;; } render() { const status = 'Next player: X'; return ( &lt;div&gt; &lt;div className=\"status\"&gt;{status}&lt;/div&gt; &lt;div className=\"board-row\"&gt; {this.renderSquare(0)} {this.renderSquare(1)} {this.renderSquare(2)} &lt;/div&gt; &lt;div className=\"board-row\"&gt; {this.renderSquare(3)} {this.renderSquare(4)} {this.renderSquare(5)} &lt;/div&gt; &lt;div className=\"board-row\"&gt; {this.renderSquare(6)} {this.renderSquare(7)} {this.renderSquare(8)} &lt;/div&gt; &lt;/div&gt; ); }} 在这里 123renderSquare(i) { return &lt;Square num={i}/&gt;;} 相当于进行了参数传递 而在下层组件square中进行调用使用了这种方法 12345return ( &lt;button className=\"square\"&gt; {this.props.num} &lt;/button&gt; );","link":"/blog/2018/08/03/React%20%E7%BB%84%E4%BB%B6%E4%BC%A0%E5%8F%82/"},{"title":"Recursive List 类题目的解法","text":"举个栗子： Given a linked list, swap every two adjacent nodes and return its head. You may not modify the values in the list’s nodes, only nodes itself may be changed. Example: 1Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. 12345678910111213141516171819202122/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */class Solution { public ListNode swapPairs(ListNode head) { if(head == null) return null; return swap(head); } public ListNode swap(ListNode n){ if(n == null || n.next == null) return n; ListNode node = n.next; n.next = swap(node.next); node.next = n; return node; }} 像这道题里需要交换顺序，那么recursive应该怎么设计？ 首先观察顺序分组 122-&gt;1 是一组4-&gt;3 是一组 然后从 1又连接到4 那么显而易见在一步recursion里要完成什么步骤 123451. 送进来的应该是第一个node2. 读取第二个node3. 第一个node的next设为下一个组的头（这里进入recursion）（所以如果我们的代码正确就不用关心返回什么了 因为肯定是下一组的头）4. 将第二个node的next设为第一个node5. 返回第二个node(非常重要 因为他是新的头) 简而言之 ，一道list的recursion题最重要的点就是关心 1. 送进来的是哪个node 2.返回去的是哪个node 以及在这一组操作之间要完成哪些步骤。只要这三个对了 整个recursion就是对的 另外作为recursion，我们同样要关心base case，也就是整个循环结束的条件 因为像这题 如果list的elements是单数，那么最后就会剩下一个，而它不会有next了，所以结束条件为了不出错肯定要 n == null || n.next == null 为结束条件。那么还有一个东西很关键，当结束条件满足时应该返回什么？ 根据前面的叙述显而易见，应该返回那个单数本身，所以 应该是return n 。这样一道recursive node 题就解出来了。","link":"/blog/2019/07/10/Recursive%20%E9%A2%98%E7%9B%AE%E7%9A%84%E8%A7%A3%E6%B3%95/"},{"title":"Optimization for Directory Rendering in React","text":"In Project “File Reader” to present a file manager-like react based web page, we are facing a problem, as we required for all the files from server, the json file for 37000 files tooks server almost 6s to respond and then the web page, depend on the cpu speed, tooks almost 9s to render, that is almost 15s to wait for user. This is not a good result, for further investigation, we use console.log(&quot;performance fetch&quot;+(t1-t0)/1000) to check the performance in seconds to see each step takes how long to finish. we can see the result 12performance fetch 5.283445000000938performance data prepare 5.33125999999902 that means the data preparation - re-organized data and sorting only tooks about 0.05 s to finish , and the rendering is so much longer if we wish to finish it in initialization of the page. But here is the fact- we don’t need to render all the sub-directories as the user don’t need to see all the files, they just want to see few of them, so we change the code like this 1234567891011121314151617181920212223242526272829303132333435generateMenu=(data)=&gt;{ let dom = []; if(data.type == &quot;dir&quot;){ let list = []; if(this.state.isRender){ for(let item of data.children){ list.push(this.generateMenu(item)); } } dom.push( &lt;div className = &quot;Directory&quot; key= {data.name?data.name:defaultDirect.direct} &gt; &lt;div onClick = {this.onMenuClicked} key= {data.name?data.name+1:defaultDirect.direct+1} &gt; &lt;i className=&quot;far fa-folder-open&quot; key= {data.name?data.name+2:defaultDirect.direct+2} &gt;&lt;/i&gt; {data.name?data.name:defaultDirect.direct} &lt;/div&gt; &lt;div style={{display:'none'}} key= {data.name?data.name+3:defaultDirect.direct+3} &gt; &lt;div style={styles.fadeInLeft} key= {data.name?data.name+4:defaultDirect.direct+4} &gt; {list} &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; ); }else{ dom.push( &lt;div className = &quot;itemContainer&quot;&gt; &lt;Item_disp item = {data.data} /&gt; &lt;/div&gt; ) } return dom;} we check the isRender state before the initialization, and if state change , that means we open a menu, then we will render the all sub-directory under that. lets see the result: 1performance whole 5.466634999997041 now the render only took 0.13s , that is 69 times faster than the previous result if we want to render whole 37000 files at the initialization.","link":"/blog/2019/06/03/Optimization%20for%20directory%20rendering%20in%20React/"},{"title":"Redux Study-- Source Code","text":"IntroThis study base on the 4.0.4 version of Redux, which is implemented by TypeScript. index.tsThe entrance of the codes, providing all the components API. isCrushed() used to discover current environment is developing or production environment . if the code is compressed and under production environment , then isCrushed.name will be type of String but the name will be changed to others 123456789if ( process.env.NODE_ENV !== 'production' &amp;&amp; typeof isCrushed.name === 'string' &amp;&amp; isCrushed.name !== 'isCrushed') { warning( ... )} createStore.tscreate store provide several functions dispatchdispatch accept an action(which in Type of A) and start running the reducer and get new state 12345678910111213141516171819202122232425262728293031323334353637383940function dispatch(action: A) { if (!isPlainObject(action)) { throw new Error( ... ) } if (typeof action.type === 'undefined') { throw new Error( ... ) } if (isDispatching) { throw new Error('Reducers may not dispatch actions.') } try { // start running the reducer isDispatching = true // the new state comes from current reducer with current state and action currentState = currentReducer(currentState, action) } finally { //finish running the reducer isDispatching = false } /** * we get the new listeners here and exec them * currentListeners = nextListeners will ensure if listeners subscribe * unsubscribe during the exec, the change will happen in next dispatch * */ const listeners = (currentListeners = nextListeners) for (let i = 0; i &lt; listeners.length; i++) { const listener = listeners[i] listener() } return action } subscribesubscribe has a parameter called listener as a callback function. 1234567891011121314151617181920212223242526272829303132333435363738394041424344function subscribe(listener: () =&gt; void) { // listener should be a function if (typeof listener !== 'function') { throw new Error('Expected the listener to be a function.') } if (isDispatching) { throw new Error( ... ) }//ensure not repeatly unsubscribe let isSubscribed = true/** * This makes a shallow copy of currentListeners so we can use * nextListeners as a temporary list while dispatching. * * This prevents any bugs around consumers calling * subscribe/unsubscribe in the middle of a dispatch. */ ensureCanMutateNextListeners() // add linstener to nextListeners nextListeners.push(listener)// unsubscribe as a return function return function unsubscribe() { if (!isSubscribed) { return } if (isDispatching) { throw new Error( ... ) } // mark as unsubscribe isSubscribed = false // again prevent buts when unsubscribe using a shallow copy ensureCanMutateNextListeners() // get index of the listener we want to unsubscribe const index = nextListeners.indexOf(listener) nextListeners.splice(index, 1) currentListeners = null } } 123456// how to make a shallow copy of current listenersfunction ensureCanMutateNextListeners() { if (nextListeners === currentListeners) { nextListeners = currentListeners.slice() } } getStatesimple as its name, just get the state 123456789function getState(): S { if (isDispatching) { throw new Error( ... ) } return currentState as S } replaceReducerreplace current reducer to get new state works with code splitting hot replacement todo: more detail analysis observablea inter connect point for observable library","link":"/blog/2020/01/12/Redux%E5%AD%A6%E4%B9%A0--%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"title":"DP-Matrix Chain Order","text":"[Array] finding a number in 2D array In a 2-D array (each sub-array has same length) every row is ascending from left to right every col is ascending from up to down, find a number in this array, return a true/false to indicate found/not found 123456789101112131415161718192021222324252627282930313233public class Solution { public boolean Find(int target, int[][] array) { if (array.length == 0 || array[0].length == 0) return false; int l = array.length - 1; int k = array[0].length - 1; if (target &lt; array[0][0] || target &gt; array[l][k]) return false; for (int i = 0; i &lt;= l; i++) { if (array[i][0] &gt; target) continue; if (search(target, i, 0, k, array)) return true; } return false; } private boolean search(int target, int i, int min, int max, int[][] arr) { if (max &gt;= min) { int mid = min+(max-min) / 2; if (target == arr[i][mid]) return true; if (arr[i][mid] &gt; target) { return search(target, i, min, mid-1, arr); } else { return search(target, i, mid+1, max, arr); } } return false; }} simplest solution, to traverse trough the first col of array in $m$ time and run a binary search for each row. total time complexity is $mlogn$.","link":"/blog/2019/01/13/%5BArray%5D%20finding%20a%20number%20in%202D%20array/"},{"title":"一种基于N-Gram模型的模糊字符串相似度匹配实现","text":"一种基于N-Gram模型的模糊字符串相似度匹配实现1.什么是N-GramN-Gram，或者称为N元模型，是NLP一种方法。本文主要用于展示N-Gram在评估字符串之间的差异程度的使用方法，并提供一种基于N-Gram的Bi-Gram实现。 2.N-Gram相似度匹配的理论基础N-Gram方法主要是通过定义N-Gram距离来体现两个字符串之间的相似度。N-Gram通过将两个字符串切分成$x$个$n$长度的子串，并通过比较字串的之间的命中次数，来决定两个字符串的距离。这是因为我们认为，一个单字出现的概率与它之前的n-1个单字有关，而与任何其它单字都没有关系，那么如果我们有一个m个词组成的句子，其概率应为$$P(w_1,w_2,…w_m)$$那么根据链式法则，有$$P(w_1,w_2,…w_m) = p(w_1)×p(w_2|w_1)×p(w_3|w_2,w_1)×…p(w_m|w_1,…,w_{m-1})$$根据马尔科夫链的“无记忆”性质，我们假设这个链中的单字只与前方其$n-1$个单字有关，有$$P(w_1,w_2,…w_m) =\\prod {i=1}^{m}P(w_i|w{i-1})$$我们以Bi-Gram(即$N=2$)方法为例 以一个字符串$s$为例，假设这个s的字串为”elpsycongroo”,那么一个Bi-Gram的拆分就为 $el, lp, ps, sy, yc, co, on, ng, gr, ro, oo, o$ 我们又有一子串t,为”elpsy”,Bi-Gram拆分为$el,lp,ps,sy,y$。 以非重复的N-Gram分词来定义N-Gram距离，我们有如下公式。$$|GN_s|+|GN_t|−2×|GN_s∩GN_t|$$那么此例中$$\\underline {el},\\underline {lp},\\underline {ps},\\underline {sy},yc, co, on, ng, gr, ro, oo, o$$ $$\\underline {el},\\underline {lp},\\underline {ps},\\underline {sy},y$$ 两个字符串之间的距离计算如下：$$(12+5)-2×4=9$$将其Normalize,则$$\\frac {9-0}{17-0}=0.529$$如果两者完全一样，则值为0，如果两者完全不同，则值为1。 3.基于JavaScript的实现我们想将这个模糊相似度匹配机制用于爬虫的信息过滤中，即如果两个房子的发布信息是同一房子多次发布 ，我们能够实现将其过滤掉的功能。 Bi-Gram实现代码如下： 12345678910111213141516171819202122232425262728293031323334function biGramComp(s,t){ let sSet = new Set(); let tSet = new Set(); let sArr = s.split(\"\"); let tArr = t.split(\"\"); let hit = 0; for(let i=0;i&lt;sArr.length;i++){ let str = sArr[i]; if(sArr[i+1]!= null){ str += sArr[i+1]; } sSet.add(str); } for(let i=0;i&lt;tArr.length;i++){ let str = tArr[i]; if(tArr[i+1] != null){ str += tArr[i+1]; } tSet.add(str); } for(let item of sSet){ if(tSet.has(item)) hit++; } let size = sSet.size + tSet.size; let correlate = size - 2*hit; let normal = correlate/size; return normal; } 如果将测试字符串输入“elpsycongroo”和’”elpsy”得到答案为0.529，符合我们的预期。如果输入字符串”233”和“2333333333”则得到答案0，即两字符串为同一字符串，如果输入字符串”Basement房子”和”1234567”的答案1,即两字符串完全无关，亦符合我们的预期。","link":"/blog/2018/07/04/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8EN-Gram%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E7%B3%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D%E5%AE%9E%E7%8E%B0/"},{"title":"如何使用Android Studio自带的http解析器制作简单的静态网页爬虫APP","text":"1.使用Andriod Studio的http解析器​ Android Studio 在高版本后自带了Jsoup解析器，要使用Jsoup解析器，首先要在 scripts:build.gradle：dependencies```中加入```implementation 'org.jsoup:jsoup:1.9.2'``` ,其后，在class里可以使用```import org.jsoup.Jsoup;``` 导入。Android Studio 默认关闭了APP的网络访问，所以要在```manifests:AndroidManifest.xml```1234中加入两行代码```&lt;usespermissionandroid:name=\"android.permission.INTERNET\" /&gt; android:name12345678910111213&lt;!-- more --&gt; #### 2.获取以及解析静态网页##### （1）获取网页 首先创建一个```private Document getDocument(Stirng url)``` ,以Math1310的课程网页为例，令```String url = &quot;http://math.yorku.ca/~lishu3/math1310.html&quot; ``` 此时，构建一个简单的 try catch模块。```java try{ return Jsoup.connect(url).timeout(5000).get(); }catch (IOException e){ e.printStackTrace(); } 需要注意的是，Jsoup.connect后需要加get()才能取得解析结果，而timeout则要加在两者之间。 这个function 返回的Document格式是Jsoup独有的格式。所以需要导入import org.jsoup.nodes.Document才不会报错。 （2）解析网页首先分析网页结构 需要抓取的结构通过分析我们可以看出位于&lt;ul&gt;&lt;/ul&gt;内 , 此时令Document doc = getDocument(url) 添加Jsoup选择器Element ul = doc.select(&quot;ul&quot;).first() 这段代码的意思是从整个被解析出的html文件中选取第一个ul作为Element。需要注意的是，这个网页中含有两个&lt;ul&gt;element 而如果不使用.first()去选择第一个ul作为element则必须使用Elements作为数据容器。 此时我们已经选择第一个&lt;ul&gt;作为目标，使用 Elements elements= ul.select(&quot;li&quot;);选取ul内所有的li作为目标。 迭代循环并输出每一个element的内容。 3.多线程更新View在Android中，原始的主线程不允许参与与网络有关的模块运行，因为非常容易造成卡顿，那么要运行刚刚的程序，必须开启一个新的线程。 12345new Thread(){ @Override public void run(){ }}.start(); 而如果要更新View,则必须使用Handler才能进行 123Handler handler=new Handler();以及handler.post(); 但是如果直接使用两者来setContentoftextView就会遇到一个问题：报错 1android.view.ViewRootImpl$CalledFromWrongThreadException: Only the original thread that created a view hierarchy can touch its views. 翻译过来就是，只有创建view的那个线程（这里是主线程）才能跟新view,而我们创建的子线程new thread如果直接运行setContentOfTextView则会报错。 这里就要使用这种方法才能避开这个问题 1234567891011121314new Thread(){ @Override public void run(){ GetInfo info=new GetInfo(); final String output=info.getInfo(\"http://math.yorku.ca/~lishu3/math1310.html\"); handler.post(new Runnable() { @Override public void run() { setContentOftextView(R.id.outputInfo,output); } }); } }.start(); 通过在handler.post里再创建一个新的Runnable event在里面跑setContentofTextView就可以成功的避开这个问题了。 4.GGWP其他一些小的经验（1）在设置textView的时候，如果加一行textView.setMovementMethod(ScrollingMovementMethod.getInstance()); 则textview可以触屏滚动 （2）在制作这个简单的爬虫APP的时候，我并没有考虑脱离网络的情况，实际上，如果不加入判断手机是否有网络的代码，则这个APP会直接崩溃，推荐在之后的APP里加入判断手机是否有Wifi或者是数据的功能。","link":"/blog/2018/06/27/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Android%20Studio%E8%87%AA%E5%B8%A6%E7%9A%84http%E8%A7%A3%E6%9E%90%E5%88%B6%E4%BD%9C%E7%AE%80%E5%8D%95%E7%9A%84%E9%9D%99%E6%80%81%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%ABAPP/"},{"title":"DP-Matrix Chain Order","text":"Analyze QuestionThis is a simple example to understand matrix chain order in DP.As we know, all the DP problem can be solve by three basic criteria find the optimized sub problem In matrix multiplication , the cost of multiplication of matrix is different between the different multi sequence.for e.g. we have 3 matrix A: 10X100 B: 100x5 C: 5x50 then if we follow diff multiplication sequence for ((A1A2)A3) : A1A2=$10\\times 100 \\times 5 = 5000 $ A3=$10\\times5\\times50 = 2500$ tot: 7500 for(A1(A2A3)): A2A3=$100 \\times 5 \\times 50 = 25000$ A1 = $10\\times100\\times50=50000$ tot:75000 and thus its 10 times bigger for the last approach than the first approach and in DP we suppose if we already know the best solution for the matrix multiplication, for e.g. (A1A2A3A4)(A5A6A7) , if we separate them into subarrays ((A1A2A3)A4) (A5(A6A7)) the subarray it self should also be the optimized multiplication. solving the sub problems according to the book(Introduction to algorithm), if we define $m[i,j]$ as recurrence, if $i=j$ then we reach the base case as $A_{i=j}$ do not need any multiplications. and we can safely conclude that $m[i,i]=0 \\space \\forall i\\in1,2,…,n$ and we call this step 1 suppose we separate the Arrays from $k$ , that said, we separate into two groups, $A_i,A_{i+1},A_{i+2}…A_k$ and $A_{k+1},A_{k+2}..A_{j}$ where $i\\leq k &lt; j$ Then in this phase we can say the to calculate $m[i,j]$ equal to calculate the sub arrays from $A_{i..k}$ and $A_{k+1..j}$ , and also should add the minimal $k$ where cause the minimal multiplication value in this phase and we will called it $p_{i-1}p_kp_j$ So the recurrence relations should like$$m[i,j]=\\begin{cases}0&amp; \\text{i=j}\\min_{i\\leq k &lt;j}(m[i,k]+m[k+1,j]+p_{i-1}p_kp_j)&amp; \\text{n = 1}\\\\end{cases}$$ Down top solution we should doing our procedure from the base case to the beginning to get our answer. Example Matrix Dimension A1 30 x 35 A2 35 x 15 A3 15 x 5 A4 5 x 10 A5 10 x 20 A6 20 x 25 Step 1Separate into two arrays No need to find min because there is only one possibilities for each part A1A2 $30 \\times 35 \\times 15 = 15750$ A2A3 $35\\times14\\times5=2625$ A3A4 $15\\times5\\times 10=750$ A5A6 $10\\times20\\times25=5000$ Step 2Separate into three arrays A1A2A3 has two variations (A1A2)A3 or A1(A2A3) we will compare this two steps (A1A2)A3 = $15750+30\\times15\\times5=18000$ A1(A2A3)=$2625+30\\times35\\times5=7875$ for smaller part we choose later one same to other part A2(A3A4) = $750+15\\times10\\times20=6000$ (A2A3)A4=$2625+35\\times5\\times10=4375$ choose 4375 A3(A4A5)=$1000+15\\times5\\times20=2500$ (A3A4)A5=$750+25\\times10\\times20=3750$ choose 2500 A4(A5A6)=$5000+5\\times10\\times25=6250$ (A4A5)A6 = $1000+5\\times20\\times25=3500$ choose 3500 Step3we proceed these steps from separate into 1 to 6 until reach the final one: A1..A6 separation A1(A2A3A4A5A6) : 44750 (A1A2)(A3A4A5A6): 32375 (A1A2A3)(A4A5A6): 15125 (A1A2A3A4)(A5A6):21875 (A1A2A3A4A5)A6: 30375 we choose 15125 (A1A2A3)(A4A5A6) and its sub arrays should be (A1(A2A3))((A4A5)A6)","link":"/blog/2019/11/28/DP-Matrix-Chain-Order/"},{"title":"一个用于爬取YorkBBS住房信息的NodeJS动态网页爬虫","text":"1.目的这个project用于爬取YorkBBS上完整的住房信息，并将其生成一个html，放入apache2的服务器下，使之可以在任何地方查看筛选好的信息。 2.网页解析以及数据接口获取YorkBBS采用的是动态网页加载，如果采用http解析器直接采集DOM结构的话，只能得到一个参杂了JS代码的HTML结构。这里有两个思路： 使用诸如PhantomnJS之类的库，等待网页加载完成才进行DOM读取 使用Chrome DevTool,通过Network功能查找和服务器端的通讯从而判断出数据的API接口，并通过伪造Header获取相应的信息 这里我们采用第二个思路，首先打开Chrome DevTool 的Network标签，刷新目标网页，观察和服务器目标中的通讯。我们很快发现了这个Header: 12345Request URL: http://house.yorkbbs.ca/house.search/api/Rent/FilterRequest Method: POSTStatus Code: 200 OKRemote Address: 104.25.35.111:80Referrer Policy: no-referrer-when-downgrade ​ 通过Response我们能够确认这是我们的目标信息： 1{\"success\":true,\"result\":{\"rowCount\":4289,\"pageIndex\":1,\"pageCount\":215,\"list\":[{\"id\":4000065874,\"title\":\"DT湖边高层condo出租一单间\",\"image\":\"https://i.ybbs.ca/media/large/37198eafc3d9d3ecda84ea4b9ad26b9c.jpeg\",\"province\":\"Ontario\",\"city\":\"Toronto\",\"region\":\"Toronto\",\"intersection1\":\"spadina\",\"intersection2\":\"fortyork\",\"price\":1100.0,\"liveInDate\":\"2018-09-01T00:00:00\",\"isCanLiveInNow\":false,\"userType\":0,\"userId\":840509,\"userName\":\"larrrrry0721\",\"refreshTime\":\"4分钟前更新\",\"linkMan\":\"larrrrry0721\",\"details\":\"禁养宠物 | 禁室内抽烟 | 少煮食 | 有家具 | 游泳池 | 健身房\",\"detailList\":{\"1001\":[\"Condo\"],\"1002\":[\"单间\"],\"1003\":[\"3房\"],\"1004\":[\"1厅\"],\"1005\":[\"无车位\"],\"1008\":[\"3个月\"],\"1010\":[\"禁养宠物\",\"禁室内抽烟\",\"少煮食\",\"有家具\",\"游泳池\",\"健身房\"]},\"enName\":\"\",\"priceNegotiable\":false,\"orderNum\":1000}},\"error\":null} 那么下一步是如何获取这些信息。 要获取相应的信息，我们只需要和正常对API接口进行请求一样就可以了，这里唯一的区别是我们并不知道该使用什么参数作为请求。再次检查Header,可以发现Header有个 Request Payload的属性，那么通过查看里面的内容。 123{ \"options\": []} 只要把这个payload加入对网址的请求当中，就可以获得如上图的信息了。 那么如何加入更多的参数选项？首先要试探新的参数，随意改变一个参数，再次对网页请求response。 123456789{ \"options\": [], \"regionId\": null, \"orderBy\": 0, \"pageIndex\": 2, \"pageSize\": 20, \"isAsc\": false, \"keyword\": \"\"} 此时可以看到参数变化了，这意味着我们可以随意改变这些参数进行符合我们期望的请求。 我们的目的是一次性抓取YorkBBS上所有的住房信息，那么只要简单的将pageIndex改为1并且把pageSize改为超出目前所有住房信息的数量，就能轻松一次性抓取论坛上所有的住房信息。 首先载入NodeJS上的http解析器superagent。const request = require('superagent') 使用superagent的promise结构进行http请求 12345request .post(url) .type('application/json') .send({\"options\": [],\"regionId\": null,\"orderBy\": 0,\"pageIndex\": 1,\"pageSize\": 50,\"isAsc\": false,\"keyword\": \"北约克\"}) // request payload .then((res)=&gt;res.toJSON()) 这里.type决定了请求的类型，而.send则决定了payload。在取得response后将response转为Json结构以供下一步使用。 3.读取并解析和过滤信息1.读取信息因为superagent采用了promise的结构，所以上一步resolve的结果直接传入下一步。 在这里我们观察json的结构，可以发现有 1text:\"{\"success\":true,\"result\":{\"rowCount\":623,\"pageIndex\":1,\"pageCount\":623,\"list\":[{\"id\":4000063147,\"title\":\"多伦多北约克屋美价廉独立屋长短期出租\",\"image\":\"https://i.ybbs.ca/media/large/2910b4198098b5aefa560a6340f4703d.jpeg\",\"province\":\"Ontario\",\"city\":\"Toronto\",\"region\":\"Toronto\",\"intersection1\":\"bayview\",\"intersection2\":\"finch\",\"price\":1190.0,\"liveInDate\":null,\"isCanLiveInNow\":true,\"userType\":0,\"userId\":112843,\"userName\":\"cmidc\",\"refreshTime\":\"44分钟前更新\",\"linkMan\":\"cmidc\",\"details\":\"单身女性 | 单身男性 | 夫妻情侣 | 三口之家\",\"detailList\":{\"1001\":[\"独立屋\"],\"1002\":[\"套间\"],\"1003\":[\"1房\"],\"1004\":[\"1厅\"],\"1005\":[\"1车位\"],\"1006\":[\"独卫\"],\"1008\":[\"不限制\"],\"1009\":[\"单身女性\",\"单身男性\",\"夫妻情侣\",\"三口之家\"],\"1010\":[\"禁养宠物\",\"禁室内抽烟\",\"有家具\",\"可短租\",\"包水电\"]},\"enName\":\"\",\"priceNegotiable\":false,\"orderNum\":1000}]},\"error\":null}\" 如果直接调用上一步的res.text,则实际得到的是一个String 而不是json object，所以我们还要进行进一步转换。将res赋值为res.text,并使用Json.parse(res)将其转为json object。 2.解析信息此时，我们可以发现这个json object中包含数个我们感兴趣的信息。 123456789101112131415161718192021city: \"Toronto\"detailList: Object {1001: Array(1), 1002: Array(1), 1003: Array(1), …}details: \"单身女性 | 单身男性 | 夫妻情侣 | 三口之家\"enName: \"\"id: 4000063147image: \"https://i.ybbs.ca/media/large/2910b4198098b5aefa560a6340f4703d.jpeg\"intersection1: \"bayview\"intersection2: \"finch\"isCanLiveInNow: truelinkMan: \"cmidc\"liveInDate: nullorderNum: 1000price: 1190priceNegotiable: falseprovince: \"Ontario\"refreshTime: \"44分钟前更新\"region: \"Toronto\"title: \"多伦多北约克屋美价廉独立屋长短期出租\"userId: 112843userName: \"cmidc\"userType: 0 比如detailList,title,region,city。下一步我们将获取这些信息并将其过滤。 3.过滤信息我们采取正则表达式的方法来判断object中是否有包含我们感兴趣的信息。 这里先建立过滤规则： 1234let pattern1 = new RegExp(\"整层\",\"i\"); //1002 整层let pattern2 = new RegExp(\"[23]房\",\"i\"); //1003 3/2 房let pattern3 = new RegExp(\"1厅\",\"i\"); //1004 1厅let pattern4 = new RegExp(\"无车位\",\"i\"); //1005 1车位 这里’”i”意味着无视大小写，而[23]房意味着选取2或者3房的条件，当我们面临多个过滤条件的时候可以采用|隔开条件。 通过pattern.test(obj)我们可以得到一个true或者false的boolean。 但是我们可以发现只有refreshTime这一行是比较特殊的数据，当时间小于1小时的时候，它会返回xx分钟前更新，而当时间小于一天的时候它会返回xx小时前更新，当时间大于1天小于7天的时候它会返回xx天前更新，而当时间大于7天时，它会返回2018年x月x日 更新。 这使得数据解析变得困难，所幸我们的筛选条件是只需要30天以内的信息，那么我们关注的是最后一种数据格式。这里我们使用JS自带的Date()方法来进行数据比较。 123456789101112131415161718function dateMatch(date){ let pattern = new RegExp(\"分|天|小时\"); if(pattern.test(date)){ return true; //7天内的时间直接返回true }else{ let end = date.slice(0,-4); end=end.replace(/[\\u4e00-\\u9fa5]/g,'-'); let olddate = Date.parse(end); //getMillis from that day let today = Date.parse(Date()); //getMillis from today let days = Math.floor((today-olddate)/86400000); if(days&lt;=30){ //只取得30天内的信息 return true; }else{ return false; } } 这里我们首先用正则方法筛选掉分，小时和天的数据，只留下日期格式。首先将多余的日 更新字符去掉，然后使用replace方法替换中文的部分成-。这样数据就是标准的日期格式了。然后使用JS自带的Date.parse方法就能够得到一个从1971年1月1日到该天数的毫秒数。同理，使用Date()方法取得现在的时间，并将其转换为毫秒数，let days = Math.floor((today-olddate)/86400000);这里86400000是一天的毫秒数。这样我们就得到了天数，将其与条件比较，不符合的抛弃。 4.生成HTML此时符合条件的数据已经筛选完毕，将其生成HTML。","link":"/blog/2018/07/02/%E4%B8%80%E4%B8%AA%E7%94%A8%E4%BA%8E%E7%88%AC%E5%8F%96YorkBBS%E4%BD%8F%E6%88%BF%E4%BF%A1%E6%81%AF%E7%9A%84NodeJS%E5%8A%A8%E6%80%81%E7%BD%91%E9%A1%B5%E7%88%AC%E8%99%AB/"},{"title":"Hands On ML","text":"Chapter 2 End to End ML ProjectSelected Performance MeasureRoot Measure Square Error$$RMSE(X,h)=\\sqrt{\\frac{1}{m}(h(x^i)-y^i)^2}$$generally the preferred performance measure for regression tasks Mean Absolute Error$$MAE(X,h) =\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}|h(x^i)-y^i|$$If there are many outlier districts Check assumptionsmake sure the task is about regression or classification, actually, if a task is to put the exist data into exist category , it is the classification task. Check dataset123456import pandas as pdhousing = load_housing_data()housing.head() // describe first 4 datas of the datasethousing.info()// describe numerical types of the datasethouding.describe()// describe the distribution of datas Create Test Setwe have to pick randomly for a subset of raw data as the test set to prevent overfitting. However there are some considerations If we purely use random method , next time we run the code , the test set may pick up some samples from learning set. we have to ensure our test set have cover all kinds of data (near) equally. So the basic way to pick up random data and ensure the same data picked up next time we run the code is using hash function by ourselves , or using library like scikit. 1train_test_split(housing,test_size=0.2,random_state=42) and we have to shuffle the test data to fit all the category. 1split = StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42) Data VisualizationA good way to check the data is visualization for data correlations we can visual the data in combination way: the Longitude Latitude and population in circle, also with the price 1234housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.4, s=housing[\"population\"]/100,label=\"population\",figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True,)plt.legend() Looking for Correlationswe can use housing.corr() looking for linear correlation ships between one to several data sets. in this case, is the median house value to all the sets of data. The correlations may go completely wrong on none linear relations . Another way to check correlation between attributes is to use scatter_matrix from pandas library 1housing.plot(kind=\"scatter\",x=\"median_income\",y=\"median_house_value\",alpha=0.1) Try out Attribute Combinationswe can add new attribute to the data in need lets add rooms per household, bedrooms per room and population per household to data 123housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]housing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]/housing[\"total_rooms\"]housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"] and show it using the correlation method again 12corr_matrix = housing.corr()corr_matrix[\"median_house_value\"].sort_values(ascending=False) Preparing DataInstead of doing it manually, we do it using functions. create a copy of training set from original one. 12housing = strat_train_set.drop(\"median_house_value\",axis = 1)housing_labels = strat_train_set[\"median_house_value\"].copy() Data Cleaningmost of the machine learning stuff cannot work with missing data, so to duel with this there are three optins get rid of the corresponding districts get rid of whole attribute set the value to some value For these options, we can use dropna(), drop() , and fillna() to accomplish task. 1234housing.dropna(subset=[&quot;total_bedrooms&quot;]) //option1housing.drop(&quot;total_bedrooms&quot;,axis=1) //option2median = housing[&quot;total_bedrooms&quot;].median() //option3housing[&quot;total_bedrooms&quot;].fillna(median,inplace=True) If we choose to use option 3 this time by using imputer API in scikit . 12from sklearn.preprocessing import Imputerimputer = Imputer(strategy=\"median\") now fit the imputer to tanning data 12housing_num = housing.drop(\"ocean_proximity\",axis=1)imputer.fit(housing_num) to check the values are missing we have to check the result of fit in statistics_ instance, and we also want to ensure there is no missing values , so also apply imputer to all the numerical attributes. now we can use the trained imputer to replace the missing vlaues 12x = imputer.transform(housing_num)housing_tr = pd.DataFrame(x,columns=housing_num.columns) The result can put into pandas data frame 1housing_tr = pd.DataFrame(x,columns=housing_num.columns) Handling Text and Categorical Attributesfor none numerical attribute we have two tries , either abandoned it or change it from text to numerical type. A basic encoding method is to use pandas factorize() method to mapping text to number 123housing_cat_encoded,housing_categories = housing_cat.factorize()housing_cat_encoded[:10]&gt;&gt;&gt;&gt;array([0, 0, 1, 2, 0, 2, 0, 2, 0, 0], dtype=int64) and we also can check the catagories 12housing_categories&gt;&gt;Index(['&lt;1H OCEAN', 'NEAR OCEAN', 'INLAND', 'NEAR BAY', 'ISLAND'], dtype='object') and we can notice that the encoding is not so robust as the &lt;1H OCEAN is encode into 1 and Near OCEAN is also encode into 1 , and we can tell they are not similar. Scikit provide another solution for this 12345from sklearn.preprocessing import CategoricalEncoder # or get from notebookcat_encoder = CategoricalEncoder()housing_cat_reshaped = housing_cat.values.reshape(-1,1)housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)housing_cat_1hot this will transform from text to 2-D array in one shot. Custom Transformers123456789101112131415161718192021222324from sklearn.base import BaseEstimator, TransformerMixin# get the right column indices: safer than hard-coding indices 3, 4, 5, 6rooms_ix, bedrooms_ix, population_ix, household_ix = [ list(housing.columns).index(col) for col in (\"total_rooms\", \"total_bedrooms\", \"population\", \"households\")]class CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kwargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] population_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, population_per_household]attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)housing_extra_attribs = attr_adder.transform(housing.values) Feature Scalingthe ML algorithm didn’t perform well on different scale of the numerical , for e.g. the number of rooms ranges from about 6 to 39320 while the median income only in range of 0 to 15. There are two ways to solve this situation: min-max scaling standarlizaiton min-max scaling or normalization: shift all the scale into ranging from 0 to 1. we do this by subtracting the min value and dividing by the max minus the min. standardization: first subtract the mean value, and divides by the variance so the resulting distribution has unit variance. Transformation Pipelines12345678910from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([ ('imputer', Imputer(strategy=\"median\")), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()), ])housing_num_tr = num_pipeline.fit_transform(housing_num) Select and Train modelTraining and Evaluating on the training setnow we can implemented model after all manipulating to our data. Lets try the Linear regression model. 123from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared,housing_labels) and show the results 12345some_data = housing.iloc[:5]some_labels = housing_labels.iloc[:5]some_data_prepared = full_pipeline.transform(some_data)print(\"Predictions:\", lin_reg.predict(some_data_prepared)) 12Predictions: [210644.60459286 317768.80697211 210956.43331178 59218.98886849 189747.55849879] let’s check the root mean square error 123456from sklearn.metrics import mean_squared_errorhousing_predictions = lin_reg.predict(housing_prepared)lin_mse = mean_squared_error(housing_labels, housing_predictions)lin_rmse = np.sqrt(lin_mse)lin_rmse 168628.19819848922 not very good result using the prediction compare to our original result let’s try Decision Tree Regressor model 1234from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor(random_state=42)tree_reg.fit(housing_prepared, housing_labels) and check the RMSE result 1234housing_predictions = tree_reg.predict(housing_prepared)tree_mse = mean_squared_error(housing_labels, housing_predictions)tree_rmse = np.sqrt(tree_mse)tree_rmse 10.0 0.0? Apparently we are overfitting our model, this is not a good result. Better Evaluation Using Cross-ValidationBut how could we confirmed 0.0 is due to overfitting instead this model is very “good” ? In order to distinguish this situation , we should use Cross validation method to our training set. This method will again , separate our training set randomly into test set and compare to the prediction and result. 12345from sklearn.model_selection import cross_val_scorescores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)tree_rmse_scores = np.sqrt(-scores) let’s try Decision tree again 123456def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std())display_scores(tree_rmse_scores) 12345Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027]Mean: 71407.68766037929Standard deviation: 2439.4345041191004 now we can see the RMSE is not so good now, we can confirmed the previous result is due to overfitting. Let’s try another model Random Forest Regressor 1234from sklearn.ensemble import RandomForestRegressorforest_reg = RandomForestRegressor(n_estimators=10, random_state=42)forest_reg.fit(housing_prepared, housing_labels) check is RMSE 1234housing_predictions = forest_reg.predict(housing_prepared)forest_mse = mean_squared_error(housing_labels, housing_predictions)forest_rmse = np.sqrt(forest_mse)forest_rmse 121933.31414779769 and also using the cross validation method 123456from sklearn.model_selection import cross_val_scoreforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)forest_rmse_scores = np.sqrt(-forest_scores)display_scores(forest_rmse_scores) 12345Scores: [51646.44545909 48940.60114882 53050.86323649 54408.98730149 50922.14870785 56482.50703987 51864.52025526 49760.85037653 55434.21627933 53326.10093303]Mean: 52583.72407377466Standard deviation: 2298.353351147122 now we can see the MEAN is much better than the Decision Tree’s MEAN. Fine-Tune Your ModelGrid SearchThere are may be many combinations of hyperparameter if we search manually thus we can use the API provide by scikit 1234567891011121314from sklearn.model_selection import GridSearchCVparam_grid = [ # try 12 (3×4) combinations of hyperparameters {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, # then try 6 (2×3) combinations with bootstrap set as False {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]forest_reg = RandomForestRegressor(random_state=42)# train across 5 folds, that's a total of (12+6)*5=90 rounds of training grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)grid_search.fit(housing_prepared, housing_labels) and show the result 1grid_search.best_params_ 1{'max_features': 8, 'n_estimators': 30} we can also let it show the best estimator directly 123456RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False) Also we can let it show the all results 123cvres = grid_search.cv_results_for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): print(np.sqrt(-mean_score), params) 12345678910111213141516171863669.05791727153 {'max_features': 2, 'n_estimators': 3}55627.16171305252 {'max_features': 2, 'n_estimators': 10}53384.57867637289 {'max_features': 2, 'n_estimators': 30}60965.99185930139 {'max_features': 4, 'n_estimators': 3}52740.98248528835 {'max_features': 4, 'n_estimators': 10}50377.344409590376 {'max_features': 4, 'n_estimators': 30}58663.84733372485 {'max_features': 6, 'n_estimators': 3}52006.15355973719 {'max_features': 6, 'n_estimators': 10}50146.465964159885 {'max_features': 6, 'n_estimators': 30}57869.25504027614 {'max_features': 8, 'n_estimators': 3}51711.09443660957 {'max_features': 8, 'n_estimators': 10}49682.25345942335 {'max_features': 8, 'n_estimators': 30}62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10} Randomized Searchif we are dueling with only a few parmeters then Grid Search is fine, otherwise we may consider using randomized search with RandomizedSearchCV with this method you can control the iteration numbers more combination of parameters 123456789101112from sklearn.model_selection import RandomizedSearchCVfrom scipy.stats import randintparam_distribs = { 'n_estimators': randint(low=1, high=200), 'max_features': randint(low=1, high=8), }forest_reg = RandomForestRegressor(random_state=42)rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)rnd_search.fit(housing_prepared, housing_labels) 123456789101112RandomizedSearchCV(cv=5, error_score='raise-deprecating', estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None, oob_score=False, random_state=42, verbose=0, warm_start=False), fit_params=None, iid='warn', n_iter=10, n_jobs=None, param_distributions={'n_estimators': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000002055C0CFA58&gt;, 'max_features': &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x000002057D30FCF8&gt;}, pre_dispatch='2*n_jobs', random_state=42, refit=True, return_train_score='warn', scoring='neg_mean_squared_error', verbose=0) 123cvres = rnd_search.cv_results_for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): print(np.sqrt(-mean_score), params) and shows the results 123cvres = rnd_search.cv_results_for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]): print(np.sqrt(-mean_score), params) 1234567891049150.657232934034 {'max_features': 7, 'n_estimators': 180}51389.85295710133 {'max_features': 5, 'n_estimators': 15}50796.12045980556 {'max_features': 3, 'n_estimators': 72}50835.09932039744 {'max_features': 5, 'n_estimators': 21}49280.90117886215 {'max_features': 7, 'n_estimators': 122}50774.86679035961 {'max_features': 3, 'n_estimators': 75}50682.75001237282 {'max_features': 3, 'n_estimators': 88}49608.94061293652 {'max_features': 5, 'n_estimators': 100}50473.57642831875 {'max_features': 3, 'n_estimators': 150}64429.763804893395 {'max_features': 5, 'n_estimators': 2} Ensemble MethodsAnother way to fine tune system is to combine the models that perform best, because compare to individual model this will usually perform better. Analyze the best Models and Their ErrorsFor random Forest Regressor can indicate the relative importance of each attribute for making accurate predictions. 12feature_importances = grid_search.best_estimator_.feature_importances_feature_importances 1234array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03]) and display these importance scores next to their attribute names: 123456extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solutioncat_encoder = full_pipeline.named_transformers_[\"cat\"]cat_one_hot_attribs = list(cat_encoder.categories_[0])attributes = num_attribs + extra_attribs + cat_one_hot_attribssorted(zip(feature_importances, attributes), reverse=True) 12345678910111213141516[(0.3661589806181342, 'median_income'), (0.1647809935615905, 'INLAND'), (0.10879295677551573, 'pop_per_hhold'), (0.07334423551601242, 'longitude'), (0.0629090704826203, 'latitude'), (0.05641917918195401, 'rooms_per_hhold'), (0.05335107734767581, 'bedrooms_per_room'), (0.041143798478729635, 'housing_median_age'), (0.014874280890402767, 'population'), (0.014672685420543237, 'total_rooms'), (0.014257599323407807, 'households'), (0.014106483453584102, 'total_bedrooms'), (0.010311488326303787, '&lt;1H OCEAN'), (0.002856474637320158, 'NEAR OCEAN'), (0.00196041559947807, 'NEAR BAY'), (6.028038672736599e-05, 'ISLAND')] with these info we can decide which parameter to drop. Evaluate Your System on the Test Set12345678910final_model = grid_search.best_estimator_X_test = strat_test_set.drop(\"median_house_value\", axis=1)y_test = strat_test_set[\"median_house_value\"].copy()X_test_prepared = full_pipeline.transform(X_test)final_predictions = final_model.predict(X_test_prepared)final_mse = mean_squared_error(y_test, final_predictions)final_rmse = np.sqrt(final_mse) 1final_rmse 147730.22690385927 We can compute a 95% confidence interval for the test RMSE: (A confidence interval gives an estimated range of values which is likely to include an unknown population parameter) http://www.stat.yale.edu/Courses/1997-98/101/confint.htm 123456789from scipy import statsconfidence = 0.95squared_errors = (final_predictions - y_test) ** 2mean = squared_errors.mean()m = len(squared_errors)np.sqrt(stats.t.interval(confidence, m - 1, loc=np.mean(squared_errors), scale=stats.sem(squared_errors))) 1array([45685.10470776, 49691.25001878])","link":"/blog/2019/05/10/Hands%20On%20ML%20Chap2/"}],"tags":[{"name":"Structured Light","slug":"Structured-Light","link":"/blog/tags/Structured-Light/"},{"name":"Seminar","slug":"Seminar","link":"/blog/tags/Seminar/"},{"name":"TCP","slug":"TCP","link":"/blog/tags/TCP/"},{"name":"java","slug":"java","link":"/blog/tags/java/"},{"name":"DP","slug":"DP","link":"/blog/tags/DP/"},{"name":"Greedy","slug":"Greedy","link":"/blog/tags/Greedy/"},{"name":"Note","slug":"Note","link":"/blog/tags/Note/"},{"name":"Redux","slug":"Redux","link":"/blog/tags/Redux/"},{"name":"Basic","slug":"Basic","link":"/blog/tags/Basic/"},{"name":"Recursive","slug":"Recursive","link":"/blog/tags/Recursive/"},{"name":"List","slug":"List","link":"/blog/tags/List/"},{"name":"Performance Optimization","slug":"Performance-Optimization","link":"/blog/tags/Performance-Optimization/"},{"name":"Array","slug":"Array","link":"/blog/tags/Array/"},{"name":"LinearSearch","slug":"LinearSearch","link":"/blog/tags/LinearSearch/"},{"name":"filter","slug":"filter","link":"/blog/tags/filter/"},{"name":"ngram","slug":"ngram","link":"/blog/tags/ngram/"},{"name":"Jsoup","slug":"Jsoup","link":"/blog/tags/Jsoup/"},{"name":"javascript","slug":"javascript","link":"/blog/tags/javascript/"},{"name":"nodejs","slug":"nodejs","link":"/blog/tags/nodejs/"},{"name":"Hands on ML","slug":"Hands-on-ML","link":"/blog/tags/Hands-on-ML/"},{"name":"Chapter 2","slug":"Chapter-2","link":"/blog/tags/Chapter-2/"}],"categories":[{"name":"Seminar","slug":"Seminar","link":"/blog/categories/Seminar/"},{"name":"Web","slug":"Web","link":"/blog/categories/Web/"},{"name":"algorithm","slug":"algorithm","link":"/blog/categories/algorithm/"},{"name":"WEB","slug":"WEB","link":"/blog/categories/WEB/"},{"name":"React","slug":"WEB/React","link":"/blog/categories/WEB/React/"},{"name":"NLP","slug":"NLP","link":"/blog/categories/NLP/"},{"name":"Crawler","slug":"Crawler","link":"/blog/categories/Crawler/"},{"name":"Andriod","slug":"Crawler/Andriod","link":"/blog/categories/Crawler/Andriod/"},{"name":"ML","slug":"ML","link":"/blog/categories/ML/"}]}